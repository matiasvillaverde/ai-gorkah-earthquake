{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "First we import the data from the csv file. We use the pandas library to read the csv file and store it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train values shape:  (260601, 39)\n",
      "Train labels shape:  (260601, 2)\n",
      "Test values shape:  (86868, 39)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import importer\n",
    "\n",
    "raw_train_values, raw_train_labels, raw_test_values = importer.import_data(directory=\"../Data\")\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(\"Train values shape: \", raw_train_values.shape)\n",
    "print(\"Train labels shape: \", raw_train_labels.shape)\n",
    "print(\"Test values shape: \", raw_test_values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (260601, 81)\n",
      "Test data shape:  (86868, 81)\n",
      "Train data columns:  Index(['building_id', 'geo_level_1_id_0', 'geo_level_1_id_1',\n",
      "       'geo_level_1_id_2', 'geo_level_1_id_3', 'geo_level_1_id_4',\n",
      "       'geo_level_2_id_0', 'geo_level_2_id_1', 'geo_level_2_id_2',\n",
      "       'geo_level_2_id_3', 'geo_level_2_id_4', 'geo_level_2_id_5',\n",
      "       'geo_level_2_id_6', 'geo_level_2_id_7', 'geo_level_2_id_8',\n",
      "       'geo_level_2_id_9', 'geo_level_2_id_10', 'geo_level_3_id_0',\n",
      "       'geo_level_3_id_1', 'geo_level_3_id_2', 'geo_level_3_id_3',\n",
      "       'geo_level_3_id_4', 'geo_level_3_id_5', 'geo_level_3_id_6',\n",
      "       'geo_level_3_id_7', 'geo_level_3_id_8', 'geo_level_3_id_9',\n",
      "       'geo_level_3_id_10', 'geo_level_3_id_11', 'geo_level_3_id_12',\n",
      "       'geo_level_3_id_13', 'count_floors_pre_eq', 'age', 'area_percentage',\n",
      "       'height_percentage', 'land_surface_condition_0',\n",
      "       'land_surface_condition_1', 'foundation_type_0', 'foundation_type_1',\n",
      "       'foundation_type_2', 'roof_type_0', 'roof_type_1',\n",
      "       'ground_floor_type_0', 'ground_floor_type_1', 'ground_floor_type_2',\n",
      "       'other_floor_type_0', 'other_floor_type_1', 'other_floor_type_2',\n",
      "       'position_0', 'position_1', 'position_2', 'plan_configuration_0',\n",
      "       'plan_configuration_1', 'plan_configuration_2', 'plan_configuration_3',\n",
      "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
      "       'has_superstructure_stone_flag',\n",
      "       'has_superstructure_cement_mortar_stone',\n",
      "       'has_superstructure_mud_mortar_brick',\n",
      "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
      "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
      "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
      "       'legal_ownership_status_0', 'legal_ownership_status_1',\n",
      "       'legal_ownership_status_2', 'count_families', 'has_secondary_use',\n",
      "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
      "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
      "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
      "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
      "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
      "      dtype='object')\n",
      "Test data columns:  Index(['building_id', 'geo_level_1_id_0', 'geo_level_1_id_1',\n",
      "       'geo_level_1_id_2', 'geo_level_1_id_3', 'geo_level_1_id_4',\n",
      "       'geo_level_2_id_0', 'geo_level_2_id_1', 'geo_level_2_id_2',\n",
      "       'geo_level_2_id_3', 'geo_level_2_id_4', 'geo_level_2_id_5',\n",
      "       'geo_level_2_id_6', 'geo_level_2_id_7', 'geo_level_2_id_8',\n",
      "       'geo_level_2_id_9', 'geo_level_2_id_10', 'geo_level_3_id_0',\n",
      "       'geo_level_3_id_1', 'geo_level_3_id_2', 'geo_level_3_id_3',\n",
      "       'geo_level_3_id_4', 'geo_level_3_id_5', 'geo_level_3_id_6',\n",
      "       'geo_level_3_id_7', 'geo_level_3_id_8', 'geo_level_3_id_9',\n",
      "       'geo_level_3_id_10', 'geo_level_3_id_11', 'geo_level_3_id_12',\n",
      "       'geo_level_3_id_13', 'count_floors_pre_eq', 'age', 'area_percentage',\n",
      "       'height_percentage', 'land_surface_condition_0',\n",
      "       'land_surface_condition_1', 'foundation_type_0', 'foundation_type_1',\n",
      "       'foundation_type_2', 'roof_type_0', 'roof_type_1',\n",
      "       'ground_floor_type_0', 'ground_floor_type_1', 'ground_floor_type_2',\n",
      "       'other_floor_type_0', 'other_floor_type_1', 'other_floor_type_2',\n",
      "       'position_0', 'position_1', 'position_2', 'plan_configuration_0',\n",
      "       'plan_configuration_1', 'plan_configuration_2', 'plan_configuration_3',\n",
      "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
      "       'has_superstructure_stone_flag',\n",
      "       'has_superstructure_cement_mortar_stone',\n",
      "       'has_superstructure_mud_mortar_brick',\n",
      "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
      "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
      "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
      "       'legal_ownership_status_0', 'legal_ownership_status_1',\n",
      "       'legal_ownership_status_2', 'count_families', 'has_secondary_use',\n",
      "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
      "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
      "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
      "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
      "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import encoder\n",
    "\n",
    "fitted_enc = encoder.create_encoder(raw_train_values)\n",
    "\n",
    "train_data = encoder.encode(raw_train_values, fitted_enc)\n",
    "test_data = encoder.encode(raw_test_values, fitted_enc)\n",
    "\n",
    "\n",
    "# Print the shapes of the new data\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "\n",
    "# Print the columns of the new data\n",
    "print(\"Train data columns: \", train_data.columns)\n",
    "print(\"Test data columns: \", test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "We clean the data by removing the rows categorical data. This is a fast implementation of the data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (260601, 81)\n"
     ]
    }
   ],
   "source": [
    "import cleaner\n",
    "\n",
    "train_cleaned = cleaner.clean(train_data, raw_train_labels)\n",
    "\n",
    "# Print the shapes of the new data\n",
    "print(\"Train data shape: \", train_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import normalizer\n",
    "\n",
    "train_normalized, test_data = normalizer.normalize(train_cleaned, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitter\n",
    "\n",
    "X_train, X_val, y_train, y_val = splitter.split(train_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model\n",
    "\n",
    "We create a model using the sklearn library. We use the RandomForestClassifier to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "model = model.XGBoost(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluator\n",
    "\n",
    "accuracy, micro_f1 = evaluator.evaluate(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Micro F1 score: {micro_f1:.5f}')\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model without splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "# Split the data\n",
    "X_train = train_normalized.drop(columns=['damage_grade'])\n",
    "y_train = train_normalized['damage_grade']\n",
    "\n",
    "# Train the model\n",
    "model = model.XGBoost(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import predictor\n",
    "\n",
    "predictions = predictor.predict(model, test_data)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions with timestamp to folder Submissions\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "filename = f'predicted_{timestamp}.csv'\n",
    "\n",
    "predictions.to_csv(f'../Submissions/{filename}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
